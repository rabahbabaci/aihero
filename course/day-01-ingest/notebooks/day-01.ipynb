{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6877670-e302-4761-b615-969b2c826309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    \n",
    "    return repository_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0afcf968-46d5-460e-a0cc-c33cff13932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5e1fe7c-4073-40f4-b0dd-a0bab519ef28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1224\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n",
    "\n",
    "# print(repository_data[0].keys())     # see what fields each entry has\n",
    "# print(repository_data[0]['filename'])\n",
    "# print(repository_data[0]['content'][:300])  # preview first 300 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e90ef4-e7ab-4be0-ac63-8ad4208f4036",
   "metadata": {},
   "source": [
    "Note: This code may not work perfectly if we want to split by level 1 headings and have Python code with # comments. But in general, this is not a big problem for documentation.\n",
    "\n",
    "If we want to split by second-level headers, that's what we do:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43823f17-6cdb-4ef4-9308-2e3c1349aa1f",
   "metadata": {},
   "source": [
    "# üß† Day 2 ‚Äì Chunking and Intelligent Processing for Data\n",
    "\n",
    "In this notebook, you‚Äôll learn to prepare long documents for AI systems by *chunking* them ‚Äî breaking large text into smaller, meaningful pieces.  \n",
    "We‚Äôll implement and compare two practical methods:\n",
    "- **Sliding-window chunking** (for unstructured text)  \n",
    "- **Section-based splitting** (for markdown documents with headings)  \n",
    "and then build a **hybrid function** that picks the right one automatically.\n",
    "\n",
    "## üß© 1Ô∏è‚É£ Setup Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46f6808c-4da8-468f-8275-5cd56b713c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()   # loads OPENAI_API_KEY and other vars from .env (safe, local)\n",
    "\n",
    "import os, re, json\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9b0b2-43be-40d5-b45d-9b3c134372a0",
   "metadata": {},
   "source": [
    "## üì• 2Ô∏è‚É£ Load or Reuse Your Data\n",
    "\n",
    "If you followed Day 1, you already have your repository data stored in a list such as `evidently_docs` or `repository_data`.  \n",
    "Let‚Äôs assume you‚Äôre continuing from Day 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1866f925-49c7-4e8c-a63f-c54ca02b297c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 95\n",
      "dict_keys(['title', 'openapi', 'content', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "# Example placeholder if you don't have Day 1 loaded in this notebook:\n",
    "# from day_01 import read_repo_data\n",
    "# evidently_docs = read_repo_data(\"DataTalksClub\", \"faq\")\n",
    "\n",
    "# If you already have the data loaded from previous steps:\n",
    "print(f\"Number of docs: {len(evidently_docs)}\")\n",
    "print(evidently_docs[0].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d387497-320c-4bee-9419-1cb967b1d417",
   "metadata": {},
   "source": [
    "## ü™ü 3Ô∏è‚É£ Simple Sliding-Window Chunking\n",
    "\n",
    "Use this when the text has no headings and you simply want evenly sized overlapping chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f5e886c-448b-4a0e-b029-eb9867da1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size=2000, step=1000):\n",
    "    \"\"\"Split long text into overlapping windows.\"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i + size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac58faf5-7bf7-4a62-a3e9-07c60e049375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks created: 19\n",
      "[{'start': 0, 'chunk': '01234567890123456789'}, {'start': 10, 'chunk': '01234567890123456789'}]\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "text = \"0123456789\" * 20\n",
    "chunks = sliding_window(text, size=20, step=10)\n",
    "print(f\"Chunks created: {len(chunks)}\")\n",
    "print(chunks[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c8b98-e18b-4015-8b6e-111485dd99f6",
   "metadata": {},
   "source": [
    "## ü™∂ 4Ô∏è‚É£ Section-Based Splitting (for Markdown)\n",
    "\n",
    "Use this when your files contain headings such as `## Section Title`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f71094-ccb9-4af3-978a-1895a61568b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
